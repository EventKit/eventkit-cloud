# -*- coding: utf-8 -*-

import json
import logging
import os
import re
import shutil
import socket
import time
import traceback
from typing import List

from zipfile import ZipFile, ZIP_DEFLATED

from billiard.einfo import ExceptionInfo
from celery import signature
from celery.result import AsyncResult
from celery.utils.log import get_task_logger
from django.conf import settings
from django.contrib.gis.geos import Polygon
from django.contrib.auth.models import User

from django.core.exceptions import ObjectDoesNotExist
from django.core.mail import EmailMultiAlternatives
from django.db import DatabaseError, transaction
from django.db.models import Q
from django.template.loader import get_template
from django.utils import timezone

from eventkit_cloud.celery import app, TaskPriority
from eventkit_cloud.core.helpers import (
    sendnotification,
    NotificationVerb,
    NotificationLevel,
)

from eventkit_cloud.feature_selection.feature_selection import FeatureSelection
from eventkit_cloud.tasks import set_cache_value
from eventkit_cloud.tasks.enumerations import TaskStates
from eventkit_cloud.tasks.exceptions import CancelException, DeleteException
from eventkit_cloud.tasks.helpers import (
    Directory,
    PREVIEW_TAIL,
    add_export_run_files_to_zip,
    check_cached_task_failures,
    clean_config,
    generate_qgs_style,
    get_arcgis_metadata,
    get_archive_data_path,
    get_download_filename,
    get_export_filename,
    get_human_readable_metadata_document,
    get_metadata,
    get_provider_download_dir,
    get_provider_slug,
    get_provider_staging_dir,
    get_provider_staging_preview,
    get_run_download_dir,
    get_run_download_url,
    get_run_staging_dir,
    get_style_files,
    normalize_name,
    pickle_exception,
    progressive_kill,
)
from eventkit_cloud.tasks.metadata import metadata_tasks
from eventkit_cloud.tasks.task_process import update_progress
from eventkit_cloud.utils.auth_requests import get_cred
from eventkit_cloud.utils import overpass, pbf, s3, mapproxy, wcs, geopackage, gdalutils
from eventkit_cloud.utils.ogr import OGR
from eventkit_cloud.utils.rocket_chat import RocketChat
from eventkit_cloud.utils.stats.eta_estimator import ETA
from eventkit_cloud.tasks.task_base import EventKitBaseTask
from audit_logging.celery_support import UserDetailsBase

from eventkit_cloud.tasks.models import (
    ExportTaskRecord,
    ExportTaskException,
    DataProviderTaskRecord,
    FileProducingTaskResult,
    ExportRun,
    RunZipFile,
)
from eventkit_cloud.jobs.models import DataProviderTask
from typing import Union
import yaml


BLACKLISTED_ZIP_EXTS = [".ini", ".om5", ".osm", ".lck", ".pyc"]

# Get an instance of a logger
logger = get_task_logger(__name__)


# http://docs.celeryproject.org/en/latest/tutorials/task-cookbook.html
# https://github.com/celery/celery/issues/3270


def make_file_downloadable(
    filepath, run_uid, provider_slug=None, skip_copy=False, download_filename=None, size=None, direct=False,
):
    """ Construct the filesystem location and url needed to download the file at filepath.
        Copy filepath to the filesystem location required for download.
        @provider_slug is specific to ExportTasks, not needed for FinalizeHookTasks
        @skip_copy: It looks like sometimes (At least for OverpassQuery) we don't want the file copied,
            generally can be ignored
        @direct: If true, return the direct download URL and skip the Downloadable tracking step
        @return A url to reach filepath.
    """

    if provider_slug:
        staging_dir = get_provider_staging_dir(run_uid, provider_slug)
        run_download_dir = get_provider_download_dir(run_uid, provider_slug)
    else:
        staging_dir = get_run_staging_dir(run_uid)
        run_download_dir = get_run_download_dir(run_uid)

    run_download_url = get_run_download_url(run_uid, provider_slug)
    filename = os.path.basename(filepath)
    if download_filename is None:
        download_filename = filename

    if getattr(settings, "USE_S3", False):
        source_path = os.path.join(staging_dir, filename)
        if provider_slug:
            download_filepath = os.path.join(run_uid, provider_slug, download_filename)
        else:
            download_filepath = os.path.join(run_uid, download_filename)
        download_url = s3.upload_to_s3(source_path, download_filepath)
    else:
        make_dirs(run_download_dir)

        download_url = os.path.join(run_download_url, download_filename)

        download_filepath = os.path.join(run_download_dir, download_filename)
        if not skip_copy:
            shutil.copy(filepath, download_filepath)
    return download_url


# ExportTaskRecord abstract base class and subclasses.
class ExportTask(EventKitBaseTask):
    """
    Abstract base class for export tasks.
    """

    # whether to abort the whole provider if this task fails.
    abort_on_error = False
    name = "ExportTask"

    def __call__(self, *args, **kwargs):
        task_uid = kwargs.get("task_uid")

        try:
            task = (
                ExportTaskRecord.objects.select_related("export_provider_task__run__job")
                .select_related("export_provider_task__provider")
                .get(uid=task_uid)
            )

            check_cached_task_failures(task.name, task_uid)

            task.worker = socket.gethostname()
            task.save()

            try:
                task_state_result = args[0]
            except IndexError:
                task_state_result = None
            self.update_task_state(result=task_state_result, task_uid=task_uid)

            if TaskStates.CANCELED.value not in [
                task.status,
                task.export_provider_task.status,
            ]:
                retval = super(ExportTask, self).__call__(*args, **kwargs)

            """
            Update the successfully completed task as follows:

                1. update the time the task completed
                2. calculate the size of the output file
                3. calculate the download path of the export
                4. create the export download directory
                5. copy the export file to the download directory
                6. create the export task result
                7. update the export task status and save it
                """
            # If a task is skipped it will be successfully completed but it won't have a return value.
            # Something needs to be populated to notify the user and to skip the following steps.
            if not (retval and retval.get("result")):
                raise Exception("This task was skipped due to previous failures/cancellations.")

            try:
                add_metadata(task.export_provider_task.run.job, task.export_provider_task.provider, retval)
            except Exception:
                logger.error(traceback.format_exc())
                logger.error("Failed to add metadata.")

            # update the task
            finished = timezone.now()
            if TaskStates.CANCELED.value in [
                task.status,
                task.export_provider_task.status,
            ]:
                logging.info(
                    "Task reported on success but was previously canceled ", format(task_uid),
                )
                username = None
                if task.cancel_user:
                    username = task.cancel_user.username
                raise CancelException(task_name=task.export_provider_task.name, user_name=username)

            task.finished_at = finished
            task.progress = 100
            task.pid = -1
            # get the output
            output_url = retval["result"]
            stat = os.stat(output_url)
            size = stat.st_size / 1024 / 1024.00
            # construct the download_path
            parts = output_url.split("/")
            filename = parts[-1]
            provider_slug = parts[-2]
            run_uid = parts[-3]
            name, ext = os.path.splitext(filename)
            if provider_slug == "run":
                event = normalize_name(task.export_provider_task.run.job.event)
                download_filename = get_download_filename(name, ext, additional_descriptors=[event, "eventkit"])
            else:
                download_filename = get_download_filename(name, ext, data_provider_slug=provider_slug)

            # construct the download url
            skip_copy = task.name == "OverpassQuery"
            download_url = make_file_downloadable(
                output_url,
                run_uid,
                provider_slug=provider_slug,
                skip_copy=skip_copy,
                download_filename=download_filename,
            )

            # save the task and task result
            result = FileProducingTaskResult.objects.create(filename=filename, size=size, download_url=download_url)

            task.result = result
            task.status = TaskStates.SUCCESS.value
            task.save()
            retval["status"] = TaskStates.SUCCESS.value
            retval["file_producing_task_result_id"] = result.id
            return retval
        except CancelException:
            return {"status": TaskStates.CANCELED.value}
        except Exception as e:
            tb = traceback.format_exc()
            logger.error("Exception in the handler for {}:\n{}".format(self.name, tb))
            from billiard.einfo import ExceptionInfo

            einfo = ExceptionInfo()
            result = self.task_failure(e, task_uid, args, kwargs, einfo)
            return result

    @transaction.atomic
    def task_failure(self, exc, task_id, args, kwargs, einfo):
        """
        Update the failed task as follows:

            1. pull out the ExportTaskRecord
            2. update the task status and finish time
            3. create an export task exception
            4. save the export task with the task exception
            5. run export_task_error_handler if the run should be aborted
               - this is only for initial tasks on which subsequent export tasks depend
        """
        # TODO: If there is a failure before the task was created this will fail to run.
        try:
            task = ExportTaskRecord.objects.get(uid=task_id)
            task.finished_at = timezone.now()
            task.save()
        except Exception:
            logger.error(traceback.format_exc())
            logger.error(
                "Cannot update the status of ExportTaskRecord object: no such object has been created for "
                "this task yet."
            )
        ete = ExportTaskException(task=task, exception=pickle_exception(einfo))
        ete.save()
        if task.status != TaskStates.CANCELED.value:
            task.status = TaskStates.FAILED.value
            task.save()
            logger.debug("Task name: {0} failed, {1}".format(self.name, einfo))
            if self.abort_on_error:
                export_provider_task = DataProviderTaskRecord.objects.get(tasks__uid=task_id)
                fail_synchronous_task_chain(data_provider_task_uid=export_provider_task.uid)
                run = export_provider_task.run
                stage_dir = kwargs["stage_dir"]
                export_task_error_handler(run_uid=str(run.uid), task_id=task_id, stage_dir=stage_dir)
            return {"status": TaskStates.FAILED.value}
        return {"status": TaskStates.CANCELED.value}

    def update_task_state(self, result=None, task_status=TaskStates.RUNNING.value, task_uid=None):
        """
        Update the task state and celery task uid.
        Can use the celery uid for diagnostics.
        """
        result = result or {}
        started = timezone.now()

        try:
            task = ExportTaskRecord.objects.get(uid=task_uid)
            celery_uid = self.request.id
            if not celery_uid:
                raise Exception("Failed to save celery_UID")
            task.celery_uid = celery_uid
            task.save()
            result = parse_result(result, "status") or []
            if TaskStates.CANCELED.value in [
                task.status,
                task.export_provider_task.status,
                result,
            ]:
                logging.info("canceling before run %s", celery_uid)
                task.status = TaskStates.CANCELED.value
                task.save()
                raise CancelException(task_name=task.export_provider_task.name)
            # The parent ID is actually the process running in celery.
            task.pid = os.getppid()
            task.status = task_status
            task.export_provider_task.status = TaskStates.RUNNING.value
            task.started_at = started
            task.save()
            task.export_provider_task.save()
            logger.debug("Updated task: {0} with uid: {1}".format(task.name, task.uid))
        except DatabaseError as e:
            logger.error("Updating task {0} state throws: {1}".format(task_uid, e))
            raise e


class AbortOnErrorTask(ExportTask):
    abort_on_error = True


class FormatTask(ExportTask):
    """
    A class to manage tasks which are desired output from the user, and not merely associated files or metadata.
    """

    name = "FormatTask"
    display = True


class ZipFileTask(FormatTask):
    def __call__(self, *args, **kwargs):
        run = ExportRun.objects.get(uid=kwargs["run_uid"])
        if kwargs["run_zip_file_uid"]:
            run_zip_file = RunZipFile.objects.get(uid=kwargs["run_zip_file_uid"])
            run_zip_file.run = run
        else:
            run_zip_file = RunZipFile.objects.create(run=run)
            kwargs["run_zip_file_uid"] = run_zip_file.uid

        retval = super(ZipFileTask, self).__call__(*args, **kwargs)

        if retval["status"] == "CANCELED":
            return retval

        if not kwargs["data_provider_task_record_uids"]:
            data_provider_task_record = DataProviderTaskRecord.objects.get(uid=kwargs["data_provider_task_record_uid"])
            data_provider_task_records = data_provider_task_record.run.provider_tasks.exclude(slug="run")
            data_provider_task_record_uids = [
                data_provider_task_record.uid for data_provider_task_record in data_provider_task_records
            ]
        else:
            data_provider_task_record_uids = kwargs["data_provider_task_record_uids"]

        if retval.get("file_producing_task_result_id"):
            data_provider_task_records = DataProviderTaskRecord.objects.filter(uid__in=data_provider_task_record_uids)
            downloadable_file = FileProducingTaskResult.objects.get(id=retval["file_producing_task_result_id"])
            run_zip_file.downloadable_file = downloadable_file
            run_zip_file.data_provider_task_records.set(data_provider_task_records)
            run_zip_file.finished_at = timezone.now()
            run_zip_file.message = "Completed"
            run_zip_file.save()
        return retval


@gdalutils.retry
def osm_data_collection_pipeline(
    export_task_record_uid,
    stage_dir,
    job_name="no_job_name_specified",
    url=None,
    slug=None,
    bbox=None,
    user_details=None,
    config=None,
    eta=None,
    projection=4326,
):
    """
    Collects data from OSM & produces a thematic gpkg as a subtask of the task referenced by export_provider_task_id.
    bbox expected format is an iterable of the form [ long0, lat0, long1, lat1 ]
    """
    # Reasonable subtask_percentages we're determined by profiling code sections on a developer workstation
    # TODO: Biggest impact to improving ETA estimates reqs higher fidelity tracking of run_query and convert

    # --- Overpass Query
    op = overpass.Overpass(
        bbox=bbox,
        stage_dir=stage_dir,
        slug=slug,
        url=url,
        job_name=job_name,
        task_uid=export_task_record_uid,
        raw_data_filename="{}_query.osm".format(job_name),
        config=config,
    )

    osm_data_filename = op.run_query(user_details=user_details, subtask_percentage=65, eta=eta)  # run the query

    # --- Convert Overpass result to PBF
    osm_filename = os.path.join(stage_dir, osm_data_filename)
    pbf_filename = os.path.join(stage_dir, "{}_query.pbf".format(job_name))
    pbf_filepath = pbf.OSMToPBF(osm=osm_filename, pbffile=pbf_filename, task_uid=export_task_record_uid).convert()

    # --- Generate thematic gpkg from PBF
    provider_slug = get_provider_slug(export_task_record_uid)
    gpkg_filepath = get_export_filename(stage_dir, job_name, projection, provider_slug, "gpkg")

    if config is None:
        logger.error("No configuration was provided for OSM export")
        raise RuntimeError("The configuration field is required for OSM data providers")

    config = clean_config(config)
    feature_selection = FeatureSelection.example(config)

    update_progress(
        export_task_record_uid, progress=67, eta=eta, msg="Converting data to Geopackage",
    )
    geom = Polygon.from_bbox(bbox)
    g = geopackage.Geopackage(
        pbf_filepath, gpkg_filepath, stage_dir, feature_selection, geom, export_task_record_uid=export_task_record_uid,
    )
    g.run(subtask_start=77, subtask_percentage=8, eta=eta)  # 77% to 85%

    # --- Add the Land Boundaries polygon layer, this accounts for the majority of post-processing time
    update_progress(export_task_record_uid, 85.5, eta=eta, msg="Clipping data in Geopackage")

    database = settings.DATABASES["feature_data"]
    in_dataset = "PG:dbname={name} host={host} user={user} password={password} port={port}".format(
        host=database["HOST"],
        user=database["USER"],
        password=database["PASSWORD"].replace("$", "\$"),
        port=database["PORT"],
        name=database["NAME"],
    )
    gdalutils.convert(
        boundary=bbox,
        input_file=in_dataset,
        output_file=gpkg_filepath,
        layers=["land_polygons"],
        fmt="gpkg",
        is_raster=False,
    )

    ret_gpkg_filepath = g.results[0].parts[0]
    assert ret_gpkg_filepath == gpkg_filepath
    update_progress(
        export_task_record_uid, progress=100, eta=eta, msg="Completed OSM data collection pipeline",
    )

    return gpkg_filepath


@app.task(name="OSM (.gpkg)", bind=True, base=FormatTask, abort_on_error=True, acks_late=True)
def osm_data_collection_task(
    self,
    result=None,
    stage_dir=None,
    run_uid=None,
    provider_slug=None,
    overpass_url=None,
    task_uid=None,
    job_name="no_job_name_specified",
    bbox=None,
    user_details=None,
    config=None,
    *args,
    **kwargs,
):
    """
    Collects data from OSM & produces a thematic gpkg as a subtask of the task referenced by export_provider_task_id.
    bbox expected format is an iterable of the form [ long0, lat0, long1, lat1 ]
    """
    logger.debug("enter run for {0}".format(self.name))
    debug_os = None

    try:
        # Uncomment debug_os to generate a simple CSV of the progress log that can be used to evaluate the accuracy
        # of ETA estimates
        # debug_os = open("{}_progress_log.csv".format(task_uid), 'w')
        eta = ETA(task_uid=task_uid, debug_os=debug_os)

        result = result or {}

        if user_details is None:
            user_details = {"username": "username not set in osm_data_collection_task"}

        gpkg_filepath = osm_data_collection_pipeline(
            task_uid,
            stage_dir,
            slug=provider_slug,
            job_name=job_name,
            bbox=bbox,
            user_details=user_details,
            url=overpass_url,
            config=config,
            eta=eta,
        )

        selection = parse_result(result, "selection")
        if selection:
            logger.debug("Calling gdalutils.convert with boundary={}, in_dataset={}".format(selection, gpkg_filepath))
            gpkg_filepath = gdalutils.convert(boundary=selection, input_file=gpkg_filepath)

        result["result"] = gpkg_filepath
        result["source"] = gpkg_filepath

        logger.debug("exit run for {0}".format(self.name))
    finally:
        if debug_os:
            debug_os.close()

    return result


def add_metadata(job, provider, retval):
    """
    Accepts a job, provider slug, and return value from a task and applies metadata to the relevant file.

    :param job:
    :param provider_slug:
    :param retval:
    :return:
    """
    result_file = retval.get("result", None)
    if result_file is None:
        return
    task = metadata_tasks.get(os.path.splitext(result_file)[1], None)
    if not provider:
        return
    if task is not None:
        task(filepath=result_file, job=job, provider=provider)


@app.task(name="ESRI Shapefile (.shp)", bind=True, base=FormatTask, acks_late=True)
def shp_export_task(
    self,
    result=None,
    run_uid=None,
    task_uid=None,
    stage_dir=None,
    job_name=None,
    user_details=None,
    projection=4326,
    *args,
    **kwargs,
):
    """
    Class defining SHP export function.
    """
    result = result or {}
    gpkg = parse_result(result, "source")
    provider_slug = get_provider_slug(task_uid)
    shapefile = get_export_filename(stage_dir, job_name, projection, provider_slug, "shp")

    try:
        ogr = OGR(task_uid=task_uid)
        out = ogr.convert(
            file_format="ESRI Shapefile",
            in_file=gpkg,
            out_file=shapefile,
            params="-lco 'ENCODING=UTF-8' -overwrite -skipfailures",
        )
        result["file_format"] = "ESRI Shapefile"
        result["result"] = out
        result["shp"] = out
        return result
    except Exception as e:
        logger.error("Exception while converting {} -> {}: {}".format(gpkg, shapefile, str(e)))
        raise


@app.task(name="Keyhole Markup Language (.kml)", bind=True, base=FormatTask, acks_late=True)
def kml_export_task(
    self,
    result=None,
    run_uid=None,
    task_uid=None,
    stage_dir=None,
    job_name=None,
    config=None,
    user_details=None,
    projection=4326,
    *args,
    **kwargs,
):
    """
    Class defining KML export function.
    """
    result = result or {}

    gpkg = parse_result(result, "source")
    provider_slug = get_provider_slug(task_uid)
    kmlfile = get_export_filename(stage_dir, job_name, projection, provider_slug, "kml")
    try:
        ogr = OGR(task_uid=task_uid)
        out = ogr.convert(file_format="KML", in_file=gpkg, out_file=kmlfile)
        result["file_extension"] = "kmz"
        result["file_format"] = "libkml"
        result["result"] = out
        result["kmz"] = out
        return result
    except Exception as e:
        logger.error("Raised exception in kml export, %s", str(e))
        raise Exception(e)


@app.task(name="SQLITE Format", bind=True, base=FormatTask, acks_late=True)
def sqlite_export_task(
    self,
    result=None,
    run_uid=None,
    task_uid=None,
    stage_dir=None,
    job_name=None,
    user_details=None,
    projection=4326,
    *args,
    **kwargs,
):
    """
    Class defining SQLITE export function.
    """
    result = result or {}

    gpkg = parse_result(result, "source")
    provider_slug = get_provider_slug(task_uid)
    sqlitefile = get_export_filename(stage_dir, job_name, projection, provider_slug, "sqlite")
    try:
        ogr = OGR(task_uid=task_uid)
        out = ogr.convert(file_format="SQLite", in_file=gpkg, out_file=sqlitefile)
        result["file_format"] = "sqlite"
        result["result"] = out
        result["sqlite"] = out
        return result
    except Exception as e:
        logger.error("Raised exception in sqlite export, %s", str(e))
        raise Exception(e)


@app.task(name="Area of Interest (.geojson)", bind=True, base=ExportTask, acks_late=True)
def output_selection_geojson_task(
    self,
    result=None,
    task_uid=None,
    selection=None,
    stage_dir=None,
    provider_slug=None,
    projection=4326,
    *args,
    **kwargs,
):
    """
    Class defining geopackage export function.
    """
    result = result or {}

    geojson_file = os.path.join(stage_dir, "{0}-{1}_selection.geojson".format(provider_slug, projection))
    if selection:
        # Test if json.
        json.loads(selection)

        from audit_logging.file_logging import logging_open

        user_details = kwargs.get("user_details")
        with logging_open(geojson_file, "w", user_details=user_details) as open_file:
            open_file.write(selection)
        result["selection"] = geojson_file
        result["result"] = geojson_file

    return result


@app.task(name="Geopackage (.gpkg)", bind=True, base=FormatTask, acks_late=True)
def geopackage_export_task(
    self,
    result=None,
    run_uid=None,
    task_uid=None,
    stage_dir=None,
    job_name=None,
    user_details=None,
    projection=4326,
    *args,
    **kwargs,
):
    """
    Class defining geopackage export function.
    """
    result = result or {}
    gpkg_in_dataset = parse_result(result, "source")

    provider_slug = get_provider_slug(task_uid)
    gpkg_out_dataset = get_export_filename(stage_dir, job_name, projection, provider_slug, "gpkg")
    selection = parse_result(result, "selection")

    gpkg = gdalutils.convert(
        fmt="gpkg",
        input_file=gpkg_in_dataset,
        output_file=gpkg_out_dataset,
        task_uid=task_uid,
        boundary=selection,
        projection=projection,
    )

    result["file_format"] = "gpkg"
    result["result"] = gpkg
    result["source"] = gpkg
    return result


@app.task(name="Geotiff (.tif)", bind=True, base=FormatTask, acks_late=True)
def geotiff_export_task(
    self, result=None, task_uid=None, stage_dir=None, job_name=None, projection=4326, config=None, *args, **kwargs,
):
    """
    Class defining geopackage export function.
    """
    result = result or {}

    gtiff_in_dataset = parse_result(result, "source")
    provider_slug = get_provider_slug(task_uid)
    gtiff_out_dataset = get_export_filename(stage_dir, job_name, projection, provider_slug, "tif")
    selection = parse_result(result, "selection")

    warp_params, translate_params = get_creation_options(config, "gtiff")

    if "tif" in os.path.splitext(gtiff_in_dataset)[1]:
        gtiff_in_dataset = f"GTIFF_RAW:{gtiff_in_dataset}"

    gtiff_out_dataset = gdalutils.convert(
        fmt="gtiff",
        input_file=gtiff_in_dataset,
        output_file=gtiff_out_dataset,
        task_uid=task_uid,
        boundary=selection,
        warp_params=warp_params,
        translate_params=translate_params,
    )

    result["file_extension"] = "tif"
    result["file_format"] = "gtiff"
    result["result"] = gtiff_out_dataset
    result["gtiff"] = gtiff_out_dataset

    return result


@app.task(name="National Imagery Transmission Format (.nitf)", bind=True, base=FormatTask, acks_late=True)
def nitf_export_task(
    self,
    result=None,
    run_uid=None,
    task_uid=None,
    stage_dir=None,
    job_name=None,
    user_details=None,
    projection=4326,
    *args,
    **kwargs,
):
    """
    Class defining nitf export function.
    """
    result = result or {}

    nitf_in_dataset = parse_result(result, "source")
    provider_slug = get_provider_slug(task_uid)
    nitf_out_dataset = get_export_filename(stage_dir, job_name, projection, provider_slug, "nitf")

    creation_options = ["ICORDS=G"]
    nitf = gdalutils.convert(
        fmt="nitf",
        input_file=nitf_in_dataset,
        output_file=nitf_out_dataset,
        task_uid=task_uid,
        creation_options=creation_options,
    )

    result["file_format"] = "nitf"
    result["result"] = nitf
    result["nitf"] = nitf
    return result


@app.task(name="Erdas Imagine HFA (.img)", bind=True, base=FormatTask, acks_late=True)
def hfa_export_task(
    self,
    result=None,
    run_uid=None,
    task_uid=None,
    stage_dir=None,
    job_name=None,
    config=None,
    user_details=None,
    projection=4326,
    *args,
    **kwargs,
):
    """
    Class defining Erdas Imagine HFA (.img) export function.
    """
    result = result or {}
    hfa_in_dataset = parse_result(result, "source")
    provider_slug = get_provider_slug(task_uid)
    hfa_out_dataset = get_export_filename(stage_dir, job_name, projection, provider_slug, "img")
    hfa = gdalutils.convert(fmt="hfa", input_file=hfa_in_dataset, output_file=hfa_out_dataset, task_uid=task_uid,)

    result["file_extension"] = "img"
    result["file_format"] = "hfa"
    result["result"] = hfa
    result["hfa"] = hfa
    return result


@app.task(name="Reprojection Task", bind=True, base=FormatTask, acks_late=True)
def reprojection_task(
    self,
    result=None,
    run_uid=None,
    task_uid=None,
    stage_dir=None,
    job_name=None,
    user_details=None,
    projection=None,
    config=None,
    *args,
    **kwargs,
):
    """
    Class defining a task that will reproject all file formats to the chosen projections.
    """
    result = result or {}
    file_format = parse_result(result, "file_format")
    selection = parse_result(result, "selection")

    if parse_result(result, "file_extension"):
        file_extension = parse_result(result, "file_extension")
    else:
        file_extension = file_format

    in_dataset = parse_result(result, "source")
    provider_slug = get_provider_slug(task_uid)
    out_dataset = get_export_filename(stage_dir, job_name, projection, provider_slug, file_extension)

    warp_params, translate_params = get_creation_options(config, file_format)

    if "tif" in os.path.splitext(in_dataset)[1]:
        in_dataset = f"GTIFF_RAW:{in_dataset}"

    reprojection = gdalutils.convert(
        fmt=file_format,
        input_file=in_dataset,
        output_file=out_dataset,
        task_uid=task_uid,
        projection=projection,
        boundary=selection,
        warp_params=warp_params,
        translate_params=translate_params,
    )

    result["result"] = reprojection

    return result


@app.task(name="WFSExport", bind=True, base=ExportTask, abort_on_error=True)
def wfs_export_task(
    self,
    result=None,
    layer=None,
    config=None,
    run_uid=None,
    task_uid=None,
    stage_dir=None,
    job_name=None,
    bbox=None,
    service_url=None,
    name=None,
    service_type=None,
    user_details=None,
    projection=4326,
    *args,
    **kwargs,
):
    """
    Class defining geopackage export for WFS service.
    """
    result = result or {}

    provider_slug = get_provider_slug(task_uid)
    gpkg = get_export_filename(stage_dir, job_name, projection, provider_slug, "gpkg")

    # Strip out query string parameters that might conflict
    service_url = re.sub(r"(?i)(?<=[?&])(version|service|request|typename|srsname)=.*?(&|$)", "", service_url,)
    query_str = "SERVICE=WFS&VERSION=1.0.0&REQUEST=GetFeature&TYPENAME={}&SRSNAME=EPSG:4326".format(layer)
    if "?" in service_url:
        if "&" != service_url[-1]:
            service_url += "&"
        service_url += query_str
    else:
        service_url += "?" + query_str

    url = service_url
    cred = get_cred(cred_var=name, url=url)
    if cred:
        user, pw = cred
        if not re.search(r"(?<=://)[a-zA-Z0-9\-._~]+:[a-zA-Z0-9\-._~]+(?=@)", url):
            url = re.sub(r"(?<=://)", "%s:%s@" % (user, pw), url)

    if bbox:
        params = "-skipfailures -spat {w} {s} {e} {n}".format(w=bbox[0], s=bbox[1], e=bbox[2], n=bbox[3])
    else:
        params = "-skipfailures"

    try:
        ogr = OGR(task_uid=task_uid)
        out = ogr.convert(file_format="GPKG", in_file=f"WFS:{url}", out_file=gpkg, params=params,)
        result["result"] = out
        result["source"] = out
        # Check for geopackage contents; gdal wfs driver fails silently
        if not geopackage.check_content_exists(out):
            logger.warning("Empty response: Unknown layer name '{}' or invalid AOI bounds".format(layer))
        return result
    except Exception as e:
        logger.error("Raised exception in wfs export: {}".format(str(e)))
        raise Exception(e)


@app.task(name="WCS Export", bind=True, base=ExportTask, abort_on_error=True, acks_late=True)
def wcs_export_task(
    self,
    result=None,
    layer=None,
    config=None,
    run_uid=None,
    task_uid=None,
    stage_dir=None,
    job_name=None,
    bbox=None,
    service_url=None,
    name=None,
    service_type=None,
    user_details=None,
    projection=4326,
    selection=None,
    *args,
    **kwargs,
):
    """
    Class defining export for WCS services
    """
    result = result or {}

    provider_slug = get_provider_slug(task_uid)
    out = get_export_filename(stage_dir, job_name, projection, provider_slug, "tif")

    eta = ETA(task_uid=task_uid)
    task = ExportTaskRecord.objects.get(uid=task_uid)

    try:
        wcs_conv = wcs.WCSConverter(
            config=config,
            out=out,
            bbox=bbox,
            service_url=service_url,
            layer=layer,
            debug=True,
            name=name,
            task_uid=task_uid,
            fmt="gtiff",
            slug=task.export_provider_task.provider.slug,
            user_details=user_details,
            eta=eta,
        )
        out = wcs_conv.convert()

        result["result"] = out
        result["source"] = out

        return result
    except Exception as e:
        logger.error("Raised exception in WCS service export: %s", str(e))
        raise Exception(e)


@app.task(name="ArcFeatureServiceExport", bind=True, base=FormatTask)
def arcgis_feature_service_export_task(
    self,
    result=None,
    task_uid=None,
    stage_dir=None,
    job_name=None,
    bbox=None,
    service_url=None,
    projection=4326,
    *args,
    **kwargs,
):
    """
    Class defining sqlite export for ArcFeatureService service.
    """
    result = result or {}

    provider_slug = get_provider_slug(task_uid)
    gpkg = get_export_filename(stage_dir, job_name, projection, provider_slug, "gpkg")
    try:
        if not os.path.exists(os.path.dirname(gpkg)):
            os.makedirs(os.path.dirname(gpkg), 6600)

        try:
            # remove any url query so we can add our own
            service_url = service_url.split("/query?")[0]
        except ValueError:
            # if no url query we can just check for trailing slash and move on
            service_url = service_url.rstrip("/\\")
        finally:
            service_url = "{}{}".format(service_url, "/query?where=objectid%3Dobjectid&outfields=*&f=json")

        if bbox:
            params = "-skipfailures -t_srs EPSG:3857 -spat_srs EPSG:4326 -spat {w} {s} {e} {n}".format(
                w=bbox[0], s=bbox[1], e=bbox[2], n=bbox[3]
            )
        else:
            params = "-skipfailures -t_srs EPSG:3857"

        ogr = OGR(task_uid=task_uid)
        out = ogr.convert(file_format="GPKG", in_file=service_url, out_file=gpkg, params=params)
        result["result"] = out
        result["source"] = out
        return result
    except Exception as e:
        logger.error("Raised exception in arcgis feature service export, %s", str(e))
        raise Exception(e)


@app.task(name="Area of Interest (.gpkg)", bind=True, base=ExportTask)
def bounds_export_task(
    self, result={}, run_uid=None, task_uid=None, stage_dir=None, provider_slug=None, projection=4326, *args, **kwargs,
):
    """
    Class defining geopackage export function.
    """
    user_details = kwargs.get("user_details")
    # This is just to make it easier to trace when user_details haven't been sent
    if user_details is None:
        user_details = {"username": "unknown-bounds_export_task"}

    run = ExportRun.objects.get(uid=run_uid)

    result_gpkg = parse_result(result, "source")
    bounds = run.job.the_geom.geojson or run.job.bounds_geojson

    gpkg = os.path.join(stage_dir, "{0}-{1}_bounds.gpkg".format(provider_slug, projection))
    gpkg = geopackage.add_geojson_to_geopackage(
        geojson=bounds, gpkg=gpkg, layer_name="bounds", task_uid=task_uid, user_details=user_details,
    )

    result["result"] = gpkg
    result["source"] = result_gpkg
    return result


@app.task(
    name="Raster export (.gpkg)", bind=True, base=FormatTask, abort_on_error=True, acks_late=True,
)
def mapproxy_export_task(
    self,
    result=None,
    layer=None,
    config=None,
    run_uid=None,
    task_uid=None,
    stage_dir=None,
    job_name=None,
    bbox=None,
    service_url=None,
    level_from=None,
    level_to=None,
    name=None,
    service_type=None,
    projection=4326,
    *args,
    **kwargs,
):
    """
    Class defining geopackage export for external raster service.
    """
    result = result or {}
    selection = parse_result(result, "selection")

    provider_slug = get_provider_slug(task_uid)
    gpkgfile = get_export_filename(stage_dir, job_name, projection, provider_slug, "gpkg")

    try:
        w2g = mapproxy.MapproxyGeopackage(
            gpkgfile=gpkgfile,
            bbox=bbox,
            service_url=service_url,
            name=name,
            layer=layer,
            config=config,
            level_from=level_from,
            level_to=level_to,
            service_type=service_type,
            task_uid=task_uid,
            selection=selection,
        )
        gpkg = w2g.convert()
        result["file_format"] = "gpkg"
        result["result"] = gpkg
        result["source"] = gpkg

        return result
    except Exception as e:
        logger.error(f"Raised exception in raster service export, {e}")
        raise e


@app.task(name="Pickup Run", bind=True, base=UserDetailsBase)
def pick_up_run_task(self, result=None, run_uid=None, user_details=None, *args, **kwargs):
    """
    Generates a Celery task to assign a celery pipeline to a specific worker.
    """
    from eventkit_cloud.tasks.task_factory import TaskFactory

    # This is just to make it easier to trace when user_details haven't been sent
    if user_details is None:
        user_details = {"username": "unknown-pick_up_run_task"}

    run = ExportRun.objects.get(uid=run_uid)
    try:
        worker = socket.gethostname()
        run.worker = worker
        run.save()
        TaskFactory().parse_tasks(worker=worker, run_uid=run_uid, user_details=user_details)
    except Exception as e:
        run.status = TaskStates.FAILED.value
        run.save()
        logger.error(str(e))
        raise
    wait_for_run(run_uid=run_uid)


def wait_for_run(run_uid: str = None) -> None:
    """

    :param run_uid: The uid of the run to wait on.
    :return: None
    """
    run = ExportRun.objects.get(uid=run_uid)
    if run.status:
        while (
            TaskStates[run.status] not in TaskStates.get_finished_states()
            and TaskStates[run.status] not in TaskStates.get_incomplete_states()
        ):
            time.sleep(10)
            run.refresh_from_db()


# This could be improved by using Redis or Memcached to help manage state.
@app.task(name="Wait For Providers", base=EventKitBaseTask, acks_late=True)
def wait_for_providers_task(result=None, apply_args=None, run_uid=None, callback_task=None, *args, **kwargs):
    if isinstance(callback_task, dict):
        callback_task = signature(callback_task)

    run = ExportRun.objects.filter(uid=run_uid).first()
    if run:
        provider_tasks = run.provider_tasks.filter(~Q(slug="run"))
        if all(
            TaskStates[provider_task.status] in TaskStates.get_finished_states() for provider_task in provider_tasks
        ):
            callback_task.apply_async(**apply_args)
        else:
            logger.warning(f"The run: {run_uid} is Waiting for other tasks to finish.")
    else:
        raise Exception("A run could not be found for uid {0}".format(run_uid))


@app.task(name="Project File (.zip)", base=ZipFileTask, acks_late=True)
def create_zip_task(
    result: dict = None,
    data_provider_task_record_uid: List[str] = None,
    data_provider_task_record_uids: List[str] = None,
    run_zip_file_uid=None,
    *args,
    **kwargs,
):
    """
    :param result: The celery task result value, it should be a dict with the current state.
    :param data_provider_task_record_uid: A data provider task record UID to zip.
    :param data_provider_task_record_uids: A list of data provider task record UIDs to zip.
    :return: The run files, or a single zip file if data_provider_task_record_uid is passed.
    """

    if not result:
        result = {}

    if not data_provider_task_record_uids:
        data_provider_task_record = DataProviderTaskRecord.objects.get(uid=data_provider_task_record_uid)
        data_provider_task_records = data_provider_task_record.run.provider_tasks.exclude(slug="run")
        data_provider_task_record_uids = [
            data_provider_task_record.uid for data_provider_task_record in data_provider_task_records
        ]

    if len(data_provider_task_record_uids) > 1:
        data_provider_task_record_slug = "run"
    elif len(data_provider_task_record_uids) == 1:
        data_provider_task_record_slug = (
            DataProviderTaskRecord.objects.select_related("provider")
            .get(uid=data_provider_task_record_uids[0])
            .provider.slug
        )

    metadata = get_metadata(data_provider_task_record_uids)
    include_files = metadata.get("include_files", None)
    if include_files:
        arcgis_dir = os.path.join(get_run_staging_dir(metadata["run_uid"]), Directory.ARCGIS.value)
        make_dirs(arcgis_dir)
        arcgis_metadata_file = os.path.join(arcgis_dir, "metadata.json")
        arcgis_metadata = get_arcgis_metadata(metadata)
        with open(arcgis_metadata_file, "w") as open_md_file:
            json.dump(arcgis_metadata, open_md_file)
        include_files += [arcgis_metadata_file]
        # No need to add QGIS file if there aren't any files to be zipped.
        include_files += [generate_qgs_style(metadata)]
        include_files += [get_human_readable_metadata_document(metadata)]
        # Need to remove duplicates from the list because
        # some intermediate tasks produce files with the same name.
        # and add the static resources
        include_files = set(include_files)

        if run_zip_file_uid:
            zip_file_name = f"{metadata['name']}-{run_zip_file_uid}.zip"
        else:
            zip_file_name = f"{metadata['name']}.zip"

        result["result"] = zip_files(
            include_files=include_files,
            run_zip_file_uid=run_zip_file_uid,
            file_path=os.path.join(
                get_provider_staging_dir(metadata["run_uid"], data_provider_task_record_slug), zip_file_name,
            ),
            static_files=get_style_files(),
        )
    else:
        raise Exception("Could not create a zip file because there were not files to include.")
    return result


@app.task(name="Finalize Export Provider Task", base=EventKitBaseTask, acks_late=True)
def finalize_export_provider_task(result=None, data_provider_task_uid=None, status=None, *args, **kwargs):
    """
    Finalizes provider task.

    Cleans up staging directory.
    Updates export provider status.
    """

    # if the status was a success, we can assume all the ExportTasks succeeded. if not, we need to parse ExportTasks to
    # mark tasks not run yet as canceled.
    result_status = parse_result(result, "status")

    with transaction.atomic():
        export_provider_task = DataProviderTaskRecord.objects.get(uid=data_provider_task_uid)
        if TaskStates[result_status] == TaskStates.CANCELED:
            export_provider_task.status = TaskStates.CANCELED.value
        elif TaskStates[result_status] != TaskStates.SUCCESS:
            export_provider_task.status = TaskStates.INCOMPLETE.value
        else:
            export_provider_task.status = TaskStates.COMPLETED.value
        export_provider_task.finished_at = timezone.now()
        export_provider_task.save()

    return result


@gdalutils.retry
def zip_files(include_files, run_zip_file_uid, file_path=None, static_files=None, *args, **kwargs):
    """
    Contains the organization for the files within the archive.
    :param include_files: A list of files to be included.
    :param run_zip_file_uid: The UUID of the zip file.
    :param file_path: An optional name for the archive.
    :param static_files: Files that are in the same location for every datapack (i.e. templates and metadata files).
    :return: The zipfile path.
    """

    if not include_files:
        logger.error("zip_file_task called with no include_files.")
        raise Exception("zip_file_task called with no include_files.")

    if not file_path:
        logger.error("zip_file_task called with no file path.")
        raise Exception("zip_file_task called with no file path.")

    run_zip_file = RunZipFile.objects.get(uid=run_zip_file_uid)

    files = [filename for filename in include_files if os.path.splitext(filename)[-1] not in BLACKLISTED_ZIP_EXTS]

    logger.debug("Opening the zipfile.")
    with ZipFile(file_path, "a", compression=ZIP_DEFLATED, allowZip64=True) as zipfile:
        if static_files:
            for absolute_file_path, relative_file_path in static_files.items():
                if "__pycache__" in absolute_file_path:
                    continue
                filename = relative_file_path
                # Support files should go in the correct directory.  It might make sense to break these files up
                # by directory and then just put each directory in the correct location so that we don't have to
                # list all support files in the future.
                basename = os.path.basename(absolute_file_path)
                if basename == "__init__.py":
                    continue
                elif os.path.basename(os.path.dirname(absolute_file_path)) == Directory.ARCGIS.value:
                    if basename in ["create_mxd.py", "ReadMe.txt"]:
                        filename = os.path.join(Directory.ARCGIS.value, "{0}".format(basename))
                    else:
                        # Put the support files in the correct directory.
                        filename = os.path.join(
                            Directory.ARCGIS.value, Directory.TEMPLATES.value, "{0}".format(basename),
                        )
                zipfile.write(absolute_file_path, arcname=filename)
        for filepath in files:
            # This takes files from the absolute stage paths and puts them in the provider directories in the data dir.
            # (e.g. staging_root/run_uid/provider_slug/file_name.ext -> data/provider_slug/file_name.ext)
            name, ext = os.path.splitext(filepath)
            provider_slug, name = os.path.split(name)
            provider_slug = os.path.split(provider_slug)[1]

            if filepath.endswith((".qgs", "ReadMe.txt")):
                # put the style file in the root of the zip
                filename = "{0}{1}".format(name, ext)
            elif filepath.endswith("metadata.json"):
                # put the metadata file in arcgis folder unless it becomes more useful.
                filename = os.path.join(Directory.ARCGIS.value, "{0}{1}".format(name, ext))
            elif filepath.endswith(PREVIEW_TAIL):
                download_filename = get_download_filename("preview", ext, data_provider_slug=provider_slug,)
                filename = get_archive_data_path(provider_slug, download_filename)
            else:
                # Put the files into directories based on their provider_slug
                # prepend with `data`

                download_filename = get_download_filename(name, ext, data_provider_slug=provider_slug)
                filename = get_archive_data_path(provider_slug, download_filename)
            run_zip_file.message = f"Adding {filename} to zip archive."
            zipfile.write(filepath, arcname=filename)

        add_export_run_files_to_zip(zipfile, run_zip_file)

        if zipfile.testzip():
            raise Exception("The zipped file was corrupted.")

    return file_path


class FinalizeRunBase(EventKitBaseTask):
    name = "Finalize Export Run"

    def run(self, result=None, run_uid=None, stage_dir=None):
        """
         Finalizes export run.

        Cleans up staging directory.
        Updates run with finish time.
        Emails user notification.
        """
        """
        """
        result = result or {}

        run = ExportRun.objects.get(uid=run_uid)
        run.status = TaskStates.COMPLETED.value
        notification_level = NotificationLevel.SUCCESS.value
        verb = NotificationVerb.RUN_COMPLETED.value
        provider_tasks = run.provider_tasks.all()

        # Complicated Celery chain from TaskFactory.parse_tasks() is incorrectly running pieces in parallel;
        #    this waits until all provider tasks have finished before continuing.
        if any(getattr(TaskStates, task.status, None) == TaskStates.PENDING for task in provider_tasks):
            finalize_run_task.retry(
                result=result, run_uid=run_uid, stage_dir=stage_dir, interval_start=4, interval_max=10,
            )

        # mark run as incomplete if any tasks fail
        if any(getattr(TaskStates, task.status, None) in TaskStates.get_incomplete_states() for task in provider_tasks):
            run.status = TaskStates.INCOMPLETE.value
            notification_level = NotificationLevel.WARNING.value
            verb = NotificationVerb.RUN_FAILED.value
        if all(getattr(TaskStates, task.status, None) == TaskStates.CANCELED for task in provider_tasks):
            run.status = TaskStates.CANCELED.value
            notification_level = NotificationLevel.WARNING.value
            verb = NotificationVerb.RUN_CANCELED.value
        finished = timezone.now()
        run.finished_at = finished
        run.save()

        # sendnotification to user via django notifications
        sendnotification(run, run.job.user, verb, None, None, notification_level, run.status)

        # send notification email to user
        site_url = settings.SITE_URL.rstrip("/")
        url = "{0}/status/{1}".format(site_url, run.job.uid)
        addr = run.user.email
        if run.status == TaskStates.CANCELED.value:
            subject = "Your Eventkit Data Pack was CANCELED."
        else:
            subject = "Your Eventkit Data Pack is ready."
        to = [addr]
        from_email = getattr(settings, "DEFAULT_FROM_EMAIL", "Eventkit Team <eventkit.team@gmail.com>")
        ctx = {"url": url, "status": run.status, "job_name": run.job.name}

        text = get_template("email/email.txt").render(ctx)
        html = get_template("email/email.html").render(ctx)
        try:
            msg = EmailMultiAlternatives(subject, text, to=to, from_email=from_email)
            msg.attach_alternative(html, "text/html")
            msg.send()
        except Exception as e:
            logger.error("Encountered an error when sending status email: {}".format(e))

        result["stage_dir"] = stage_dir
        return result

    def after_return(self, status, retval, task_id, args, kwargs, einfo):
        super(FinalizeRunBase, self).after_return(status, retval, task_id, args, kwargs, einfo)
        stage_dir = None if retval is None else retval.get("stage_dir")
        try:
            if stage_dir and os.path.isdir(stage_dir):
                if not os.getenv("KEEP_STAGE", False):
                    shutil.rmtree(stage_dir)
        except IOError or OSError:
            logger.error("Error removing {0} during export finalize".format(stage_dir))


# There's a celery bug with callbacks that use bind=True.  If altering this task do not use Bind.
# @see: https://github.com/celery/celery/issues/3723
@app.task(name="Finalize Run Task", base=FinalizeRunBase)
def finalize_run_task(result=None, run_uid=None, stage_dir=None, apply_args=None, *args, **kwargs):
    """
     Finalizes export run.

    Cleans up staging directory.
    Updates run with finish time.
    Emails user notification.
    """
    result = result or {}

    run = ExportRun.objects.get(uid=run_uid)
    run.status = TaskStates.COMPLETED.value
    verb = NotificationVerb.RUN_COMPLETED.value
    notification_level = NotificationLevel.SUCCESS.value
    provider_tasks = run.provider_tasks.exclude(slug="run")

    # mark run as incomplete if any tasks fail
    if any(getattr(TaskStates, task.status, None) in TaskStates.get_incomplete_states() for task in provider_tasks):
        run.status = TaskStates.INCOMPLETE.value
        notification_level = NotificationLevel.WARNING.value
        verb = NotificationVerb.RUN_FAILED.value
    if all(getattr(TaskStates, task.status, None) == TaskStates.CANCELED for task in provider_tasks):
        run.status = TaskStates.CANCELED.value
        verb = NotificationVerb.RUN_CANCELED.value
        notification_level = NotificationLevel.WARNING.value
    finished = timezone.now()
    run.finished_at = finished
    run.save()

    # sendnotification to user via django notifications

    sendnotification(run, run.job.user, verb, None, None, notification_level, run.status)

    # send notification email to user
    site_url = settings.SITE_URL.rstrip("/")
    url = "{0}/status/{1}".format(site_url, run.job.uid)
    addr = run.user.email
    if run.status == TaskStates.CANCELED.value:
        subject = "Your Eventkit Data Pack was CANCELED."
    else:
        subject = "Your Eventkit Data Pack is ready."
    to = [addr]
    from_email = getattr(settings, "DEFAULT_FROM_EMAIL")
    ctx = {"url": url, "status": run.status, "job_name": run.job.name}

    text = get_template("email/email.txt").render(ctx)
    html = get_template("email/email.html").render(ctx)
    try:
        msg = EmailMultiAlternatives(subject, text, to=to, from_email=from_email)
        msg.attach_alternative(html, "text/html")
        msg.send()
    except Exception as e:
        logger.error("Encountered an error when sending status email: {}".format(e))

    result["stage_dir"] = stage_dir
    return result


@app.task(name="Export Task Error Handler", bind=True, base=EventKitBaseTask)
def export_task_error_handler(self, result=None, run_uid=None, task_id=None, stage_dir=None, *args, **kwargs):
    """
    Handles un-recoverable errors in export tasks.
    """
    result = result or {}

    run = ExportRun.objects.get(uid=run_uid)
    try:
        if os.path.isdir(stage_dir):
            if not os.getenv("KEEP_STAGE", False):
                shutil.rmtree(stage_dir)
    except IOError:
        logger.error("Error removing {0} during export finalize".format(stage_dir))

    site_url = settings.SITE_URL
    url = "{0}/status/{1}".format(site_url.rstrip("/"), run.job.uid)
    addr = run.user.email
    subject = "Your Eventkit Data Pack has a failure."
    # email user and administrator
    to = [addr, settings.TASK_ERROR_EMAIL]
    from_email = getattr(settings, "DEFAULT_FROM_EMAIL", "Eventkit Team <eventkit.team@gmail.com>")
    ctx = {"url": url, "task_id": task_id, "job_name": run.job.name}
    text = get_template("email/error_email.txt").render(ctx)
    html = get_template("email/error_email.html").render(ctx)
    msg = EmailMultiAlternatives(subject, text, to=to, from_email=from_email)
    msg.attach_alternative(html, "text/html")
    msg.send()

    # Send failed DataPack notifications to specific channel(s) or user(s) if enabled.
    rocketchat_notifications = settings.ROCKETCHAT_NOTIFICATIONS
    if rocketchat_notifications:
        channels = rocketchat_notifications["channels"]
        message = f"@here: A DataPack has failed during processing. {ctx['url']}"

        client = RocketChat(**rocketchat_notifications)
        for channel in channels:
            client.post_message(channel, message)

    return result


def fail_synchronous_task_chain(data_provider_task_uid=None):
    data_provider_task_record = DataProviderTaskRecord.objects.get(uid=data_provider_task_uid)
    for export_task in data_provider_task_record.tasks.all():
        if TaskStates[export_task.status] == TaskStates.PENDING:
            export_task.status = TaskStates.FAILED.value
            export_task.save()
            kill_task.apply_async(
                kwargs={"task_pid": export_task.pid, "celery_uid": export_task.celery_uid},
                queue="{0}.priority".format(export_task.worker),
                priority=TaskPriority.CANCEL.value,
                routing_key="{0}.priority".format(export_task.worker),
            )


@app.task(name="Create preview", base=EventKitBaseTask, acks_late=True, reject_on_worker_lost=True)
def create_datapack_preview(
    result=None, run_uid=None, task_uid=None, stage_dir=None, task_record_uid=None, *args, **kwargs,
):
    """
    Attempts to add a MapImageSnapshot (Preview Image) to a provider task.
    """
    result = result or {}
    try:
        from eventkit_cloud.utils.image_snapshot import (
            get_wmts_snapshot_image,
            make_snapshot_downloadable,
            fit_to_area,
        )

        check_cached_task_failures(create_datapack_preview.name, task_uid)

        provider_task = DataProviderTask.objects.select_related("provider").get(uid=task_uid)
        provider = provider_task.provider

        provider_task_record = DataProviderTaskRecord.objects.get(uid=task_record_uid)

        export_run = ExportRun.objects.get(uid=run_uid)
        job = export_run.job

        filepath = get_provider_staging_preview(export_run.uid, provider.slug)
        make_dirs(stage_dir)
        preview = get_wmts_snapshot_image(provider.preview_url, bbox=job.extents)
        fit_to_area(preview)
        preview.save(filepath)

        download_filename = f"{run_uid}/{provider.slug}/{PREVIEW_TAIL}"
        snapshot = make_snapshot_downloadable(filepath, download_filename=download_filename, copy=True)
        provider_task_record.preview = snapshot
        provider_task_record.save()
        result["result"] = filepath

    except Exception as e:
        logger.exception(e)
    return result


@app.task(name="Cancel Export Provider Task", base=EventKitBaseTask)
def cancel_export_provider_task(
    result=None, data_provider_task_uid=None, canceling_username=None, delete=False, error=False, *args, **kwargs,
):
    """
    Cancels an DataProviderTaskRecord and terminates each subtasks execution.
    Checks if all DataProviderTasks for the Run grouping them have finished & updates the Run's status.
    """

    # There is enough over use of this class (i.e. for errors, deletions, canceling) the reason is because it had all
    # the working logic for stopping future jobs, but that can probably be abstracted a bit, and then let the caller
    # manage the task state (i.e. the task should be FAILED or CANCELED).
    result = result or {}
    data_provider_task_record = DataProviderTaskRecord.objects.get(uid=data_provider_task_uid)

    # There might not be a canceling user...
    try:
        canceling_user = User.objects.get(username=canceling_username)
    except ObjectDoesNotExist:
        canceling_user = None

    export_tasks = data_provider_task_record.tasks.all()

    # Loop through both the tasks in the DataProviderTaskRecord model, as well as the Task Chain in celery
    for export_task in export_tasks.all():
        if delete:
            exception_class = DeleteException
        else:
            exception_class = CancelException
        if TaskStates[export_task.status] not in TaskStates.get_finished_states():
            export_task.status = TaskStates.CANCELED.value
            if canceling_user:
                export_task.cancel_user = canceling_user
            export_task.save()
        # This part is to populate the UI with the cancel message.  If a different mechanism is incorporated
        # to pass task information to the users, then it may make sense to replace this.
        try:
            raise exception_class(task_name=data_provider_task_record.name, user_name=canceling_user)
        except exception_class as ce:
            einfo = ExceptionInfo()
            einfo.exception = ce
            ExportTaskException.objects.create(task=export_task, exception=pickle_exception(einfo))

        # Remove the ExportTaskResult, which will clean up the files.
        task_result = export_task.result
        if task_result:
            task_result.soft_delete()

        if int(export_task.pid) > 0 and export_task.worker:
            kill_task.apply_async(
                kwargs={"task_pid": export_task.pid, "celery_uid": export_task.celery_uid},
                queue="{0}.priority".format(export_task.worker),
                priority=TaskPriority.CANCEL.value,
                routing_key="{0}.priority".format(export_task.worker),
            )

        # Add canceled to the cache so processes can check in to see if they should abort.
        set_cache_value(
            uid=export_task.uid, attribute="status", model_name="ExportTaskRecord", value=TaskStates.CANCELED.value,
        )

    if TaskStates[data_provider_task_record.status] not in TaskStates.get_finished_states():
        if error:
            data_provider_task_record.status = TaskStates.FAILED.value
        else:
            data_provider_task_record.status = TaskStates.CANCELED.value
    data_provider_task_record.save()

    return result


@app.task(name="Cancel Run", base=EventKitBaseTask)
def cancel_run(
    result=None, export_run_uid=None, canceling_username=None, delete=False, *args, **kwargs,
):
    result = result or {}

    export_run = ExportRun.objects.get(uid=export_run_uid)

    for export_provider_task in export_run.provider_tasks.all():
        cancel_export_provider_task(
            data_provider_task_uid=export_provider_task.uid,
            canceling_username=canceling_username,
            delete=delete,
            locking_task_key="cancel_export_provider_task-{0}".format(export_provider_task.uid),
        )
    result["result"] = True
    return result


@app.task(name="Kill Task", base=EventKitBaseTask)
def kill_task(result=None, task_pid=None, celery_uid=None, *args, **kwargs):
    """
    Asks a worker to kill a task.
    """
    import celery

    result = result or {}

    if celery_uid:
        try:
            # Ensure the task is still running otherwise the wrong process will be killed
            if AsyncResult(str(celery_uid), app=app).state == celery.states.STARTED:
                # If the task finished prior to receiving this kill message it could throw an OSError.
                logger.info("Attempting to kill {0}".format(task_pid))
                # Don't kill tasks with default pid.
                if int(task_pid) > 0:
                    progressive_kill(task_pid)
            else:
                logger.info(
                    "The celery_uid {0} has the status of {1}.".format(
                        celery_uid, AsyncResult(celery_uid, app=app).state
                    )
                )
        except OSError:
            logger.info("{0} PID does not exist.".format(task_pid))
    return result


def parse_result(task_result, key=""):
    """
    Used to parse the celery result for a specific value.
    :param task_result: A str, dict, or list.
    :param key: A key to search dicts for
    :return: The value at the desired key or the original task result if not in a list or dict.
    """
    if not task_result:
        return None
    if isinstance(task_result, list):
        for result in task_result:
            if result.get(key):
                return result.get(key)
    elif isinstance(task_result, dict):
        return task_result.get(key)


def get_function(function):
    from importlib import import_module

    module, function_name = function.rsplit(".", 1)

    module_object = import_module(module)
    function_object = getattr(module_object, function_name)

    return function_object


def get_creation_options(config: str, file_format: str) -> Union[list, None]:
    """
    Gets a list of options for a specific format or returns None.
    :param config: The configuration for a datasource.
    :param file_format: The file format to look for specific creation options.
    :return: A list of creation options of None
    """
    if config:
        conf = yaml.safe_load(config) or dict()
        params = conf.get("formats", {}).get(file_format, {})
        return params.get("warp_params"), params.get("translate_params")
    return None, None


def make_dirs(path):
    try:
        os.makedirs(path)
    except OSError:
        if not os.path.isdir(path):
            raise
